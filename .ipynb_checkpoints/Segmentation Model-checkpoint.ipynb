{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883e009-1df5-4f12-8b3b-e1ace23b6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "NUM_CLASSES = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103a073-c768-46ad-9ac3-77951c756126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with information\n",
    "dataset, info = tfds.load(\"oxford_iiit_pet\", with_info=True)\n",
    "print(\"Dataset features:\", info.features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92840e2-2701-48ad-91ba-3984a9979c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample to confirm\n",
    "sample = next(iter(dataset['train'].take(1)))\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "image_sample, mask_sample = sample['image'], sample['segmentation_mask']\n",
    "\n",
    "# Check unique labels in the sample mask\n",
    "unique_labels = tf.unique(tf.reshape(mask_sample, [-1]))[0].numpy()\n",
    "print(\"Unique labels in the sample mask:\", unique_labels)\n",
    "\n",
    "# Visualize the sample mask\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(mask_sample.numpy().squeeze(), cmap='jet')\n",
    "plt.title('Sample Segmentation Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43ca19-9531-4608-98c1-04d891e749c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Extract image and mask from the data\n",
    "    image = data['image']\n",
    "    mask = data.get('segmentation_mask')\n",
    "\n",
    "    if mask is None:\n",
    "        print(\"Mask is missing. Creating a default mask of zeros.\")\n",
    "        mask = tf.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=tf.int32)  # Adjust size as needed\n",
    "    else:\n",
    "        print(\"Original Mask Shape:\", mask.shape)\n",
    "\n",
    "        # Resize the mask using nearest neighbor to preserve labels\n",
    "        mask = tf.image.resize(mask, (IMG_HEIGHT, IMG_WIDTH), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        mask = tf.cast(mask, tf.int32)  # Keep as integer labels\n",
    "\n",
    "        # Subtract 1 to make labels start from 0\n",
    "        mask = mask - 1\n",
    "        mask = tf.where(mask < 0, tf.zeros_like(mask), mask)  # Replace negative values with 0\n",
    "\n",
    "    # Resize image\n",
    "    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "    # Normalize image\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b6148-c578-497c-b717-549df1dd459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train'].map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = dataset['test'].map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f603ae-92a8-4930-9197-1c2d513977ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = tf.keras.layers.Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)\n",
    "    merge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    merge7 = tf.keras.layers.concatenate([conv3, up7], axis=3)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n",
    "    merge8 = tf.keras.layers.concatenate([conv2, up8], axis=3)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n",
    "    merge9 = tf.keras.layers.concatenate([conv1, up9], axis=3)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    # Output Layer for Multi-class Segmentation\n",
    "    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    # Compile the model with appropriate loss function and metrics\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f8c78-a8e7-431e-82fe-cc2666c6c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define steps per epoch\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "VAL_LENGTH = info.splits['test'].num_examples\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "VALIDATION_STEPS = VAL_LENGTH // BATCH_SIZE\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=10,  \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97858da2-f350-4c18-af66-3260fed27321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Predictions\n",
    "for image_batch, mask_batch in val_dataset.take(1):\n",
    "    pred_mask = model.predict(image_batch)\n",
    "    \n",
    "    # Convert predictions to class labels\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    \n",
    "    # Visualize one image, its true mask, and the predicted mask\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Input Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Input Image')\n",
    "    plt.imshow(image_batch[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # True Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('True Mask')\n",
    "    plt.imshow(tf.squeeze(mask_batch[0]), cmap='jet')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Predicted Mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.imshow(tf.squeeze(pred_mask[0]), cmap='jet')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d7f2b-1c3b-48ff-b166-71aaac0f8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "model_save_path = os.path.join(desktop_path, \"unet_model.keras\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model successfully saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967c91d-2e37-4682-bb26-1a1590c9190e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
